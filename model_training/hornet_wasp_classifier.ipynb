{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hornet and Wasp Classification Project\n",
    "\n",
    "This notebook implements and compares three different deep learning models for classifying hornets and wasps:\n",
    "- Model 1: Custom CNN from scratch\n",
    "- Model 2: Transfer Learning with pre-trained ResNet\n",
    "- Model 3: Vision Transformer (ViT)\n",
    "\n",
    "**Dataset:**\n",
    "- Vespa_crabro: 954 train, 100 validation images\n",
    "- Vespa_velutina: 1,102 train, 100 validation images\n",
    "- Vespula_sp: 1,032 train, 97 validation images\n",
    "- Total: 3,088 training, 297 validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "train_dir = 'dataset/data3000/data/train/images'\n",
    "val_dir = 'dataset/data3000/data/val/images'\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "print(f\"Classes: {class_names}\")\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Count images per class\n",
    "for split, split_dir in [('Train', train_dir), ('Validation', val_dir)]:\n",
    "    print(f\"\\n{split} set distribution:\")\n",
    "    total = 0\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(split_dir, class_name)\n",
    "        count = len(os.listdir(class_path))\n",
    "        total += count\n",
    "        print(f\"  {class_name}: {count} images\")\n",
    "    print(f\"  Total: {total} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    image_files = os.listdir(class_path)[:5]  # Take first 5 images\n",
    "    \n",
    "    for j, img_file in enumerate(image_files):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].set_title(f'{class_name}' if j == 0 else '')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image dimensions\n",
    "dimensions = []\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    for img_file in os.listdir(class_path)[:10]:  # Sample 10 images per class\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "        img = Image.open(img_path)\n",
    "        dimensions.append(img.size)\n",
    "\n",
    "dimensions_df = pd.DataFrame(dimensions, columns=['width', 'height'])\n",
    "print(\"Image dimensions statistics:\")\n",
    "print(dimensions_df.describe())\n",
    "\n",
    "# Plot dimension distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(dimensions_df['width'], bins=20, alpha=0.7, label='Width')\n",
    "ax1.set_xlabel('Width (pixels)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Image Width Distribution')\n",
    "\n",
    "ax2.hist(dimensions_df['height'], bins=20, alpha=0.7, label='Height')\n",
    "ax2.set_xlabel('Height (pixels)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Image Height Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset = ImageFolder(val_dir, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Class to index mapping: {train_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented images\n",
    "def show_augmented_images(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    \n",
    "    # Get a random sample\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    original_img = Image.open(dataset.imgs[idx][0])\n",
    "    \n",
    "    # Show original\n",
    "    axes[0, 0].imshow(original_img)\n",
    "    axes[0, 0].set_title('Original')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Show augmented versions\n",
    "    for i in range(num_samples):\n",
    "        if i == 0:\n",
    "            # Show resized version\n",
    "            img_tensor, _ = dataset[idx]\n",
    "            # Denormalize for display\n",
    "            denorm = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                                        std=[1/0.229, 1/0.224, 1/0.225])\n",
    "            img_display = denorm(img_tensor).clamp(0, 1)\n",
    "            axes[1, i].imshow(img_display.permute(1, 2, 0))\n",
    "            axes[1, i].set_title('Processed')\n",
    "        else:\n",
    "            img_tensor, _ = dataset[idx]\n",
    "            # Denormalize for display\n",
    "            denorm = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                                        std=[1/0.229, 1/0.224, 1/0.225])\n",
    "            img_display = denorm(img_tensor).clamp(0, 1)\n",
    "            axes[1, i].imshow(img_display.permute(1, 2, 0))\n",
    "            axes[1, i].set_title(f'Augmented {i}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots in first row\n",
    "    for i in range(1, num_samples):\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_augmented_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Custom CNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(256)\n",
    "        self.batch_norm5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n",
    "        \n",
    "        # Block 4\n",
    "        x = self.pool(F.relu(self.batch_norm4(self.conv4(x))))\n",
    "        \n",
    "        # Block 5\n",
    "        x = self.pool(F.relu(self.batch_norm5(self.conv5(x))))\n",
    "        \n",
    "        # Flatten and fully connected\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "model_cnn = CustomCNN(num_classes=num_classes).to(device)\n",
    "print(f\"Custom CNN parameters: {sum(p.numel() for p in model_cnn.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "output = model_cnn(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Transfer Learning with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetTransfer(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained=True):\n",
    "        super(ResNetTransfer, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Replace the final layer\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create ResNet model\n",
    "model_resnet = ResNetTransfer(num_classes=num_classes).to(device)\n",
    "print(f\"ResNet50 parameters: {sum(p.numel() for p in model_resnet.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "output = model_resnet(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, num_classes=3, img_size=224, patch_size=16, embed_dim=768, num_heads=12, num_layers=12):\n",
    "        super(SimpleViT, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Class token and positional embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, embed_dim, H//patch_size, W//patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.norm(x[:, 0])  # Use class token\n",
    "        x = self.dropout(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create ViT model (smaller version for faster training)\n",
    "model_vit = SimpleViT(num_classes=num_classes, embed_dim=384, num_heads=6, num_layers=6).to(device)\n",
    "print(f\"ViT parameters: {sum(p.numel() for p in model_vit.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "output = model_vit(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_name):\n",
    "    \"\"\"\n",
    "    Train a model and return training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_running_loss += loss.item()\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(val_loader)\n",
    "        val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
    "        \n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')\n",
    "            print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
    "            print(f'Best Val Acc: {best_val_acc:.4f}')\n",
    "            print()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "def evaluate_model(model, data_loader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return detailed metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize models\n",
    "models_dict = {\n",
    "    'Custom CNN': CustomCNN(num_classes=num_classes).to(device),\n",
    "    'ResNet50': ResNetTransfer(num_classes=num_classes).to(device),\n",
    "    'Vision Transformer': SimpleViT(num_classes=num_classes, embed_dim=384, num_heads=6, num_layers=6).to(device)\n",
    "}\n",
    "\n",
    "# Training results storage\n",
    "results = {}\n",
    "training_histories = {}\n",
    "\n",
    "# Train each model\n",
    "for model_name, model in models_dict.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if model_name == 'ResNet50':\n",
    "        # Use different learning rates for backbone and classifier\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.backbone.fc.parameters(), 'lr': learning_rate},\n",
    "            {'params': model.backbone.parameters(), 'lr': learning_rate * 0.1}\n",
    "        ])\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history, best_val_acc = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_name\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    metrics = evaluate_model(model, val_loader, class_names)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'training_time': training_time,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "    training_histories[model_name] = history\n",
    "    \n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"Final validation accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score: {metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    metrics = result['metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "        'Precision': f\"{metrics['precision']:.4f}\",\n",
    "        'Recall': f\"{metrics['recall']:.4f}\",\n",
    "        'F1-Score': f\"{metrics['f1']:.4f}\",\n",
    "        'Training Time (s)': f\"{result['training_time']:.1f}\",\n",
    "        'Parameters': f\"{sum(p.numel() for p in result['model'].parameters()):,}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Histories Comparison', fontsize=16)\n",
    "\n",
    "metrics_to_plot = ['train_loss', 'val_loss', 'train_acc', 'val_acc']\n",
    "titles = ['Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for model_name, history in training_histories.items():\n",
    "        ax.plot(history[metric], label=model_name, linewidth=2)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title.split()[1])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices Comparison', fontsize=16)\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = result['metrics']['confusion_matrix']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[idx])\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {result[\"metrics\"][\"accuracy\"]:.4f}')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics comparison\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    metrics = result['metrics']\n",
    "    values = [metrics[metric] for metric in metrics_names]\n",
    "    ax.bar(x + i * width, values, width, label=model_name, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels([m.capitalize() for m in metrics_names])\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    metrics = result['metrics']\n",
    "    values = [metrics[metric] for metric in metrics_names]\n",
    "    for j, v in enumerate(values):\n",
    "        ax.text(j + i * width, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Per-Class Performance Comparison', fontsize=16)\n",
    "\n",
    "metrics_to_show = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "for metric_idx, metric in enumerate(metrics_to_show):\n",
    "    ax = axes[metric_idx]\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for model_idx, (model_name, result) in enumerate(results.items()):\n",
    "        report = result['metrics']['classification_report']\n",
    "        values = [report[class_name][metric] for class_name in class_names]\n",
    "        ax.bar(x + model_idx * width, values, width, label=model_name, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()} by Class')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([name.replace('_', '\\n') for name in class_names], fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Examples with Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(models_dict, image_path, transform, class_names, top_k=3):\n",
    "    \"\"\"\n",
    "    Make predictions with all models and show confidence scores\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)[0]\n",
    "            \n",
    "            # Get top k predictions\n",
    "            top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "            \n",
    "            predictions[model_name] = {\n",
    "                'top_classes': [class_names[idx] for idx in top_indices.cpu().numpy()],\n",
    "                'top_probs': [prob.item() for prob in top_probs.cpu()],\n",
    "                'predicted_class': class_names[top_indices[0]],\n",
    "                'confidence': top_probs[0].item()\n",
    "            }\n",
    "    \n",
    "    return image, predictions\n",
    "\n",
    "# Get some random validation images for prediction examples\n",
    "sample_images = []\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(val_dir, class_name)\n",
    "    images = os.listdir(class_path)\n",
    "    sample_images.extend([os.path.join(class_path, img) for img in images[:2]])\n",
    "\n",
    "# Make predictions on sample images\n",
    "fig, axes = plt.subplots(len(sample_images), 4, figsize=(20, 5 * len(sample_images)))\n",
    "if len(sample_images) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for img_idx, img_path in enumerate(sample_images):\n",
    "    # Get predictions\n",
    "    image, predictions = predict_with_confidence(\n",
    "        {name: result['model'] for name, result in results.items()}, \n",
    "        img_path, val_transform, class_names\n",
    "    )\n",
    "    \n",
    "    # True label\n",
    "    true_class = os.path.basename(os.path.dirname(img_path))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[img_idx, 0].imshow(image)\n",
    "    axes[img_idx, 0].set_title(f'True Class: {true_class}', fontweight='bold')\n",
    "    axes[img_idx, 0].axis('off')\n",
    "    \n",
    "    # Show predictions for each model\n",
    "    for model_idx, (model_name, pred) in enumerate(predictions.items()):\n",
    "        ax = axes[img_idx, model_idx + 1]\n",
    "        \n",
    "        # Create prediction text\n",
    "        pred_text = f\"Prediction: {pred['predicted_class']}\\nConfidence: {pred['confidence']:.1%}\\n\\n\"\n",
    "        pred_text += \"Top 3 predictions:\\n\"\n",
    "        for class_name, prob in zip(pred['top_classes'], pred['top_probs']):\n",
    "            pred_text += f\"• {class_name}: {prob:.1%}\\n\"\n",
    "        \n",
    "        # Color based on correctness\n",
    "        color = 'green' if pred['predicted_class'] == true_class else 'red'\n",
    "        \n",
    "        ax.text(0.05, 0.95, pred_text, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor=color, alpha=0.1))\n",
    "        ax.set_title(f'{model_name}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best performing model\n",
    "best_model = max(results.items(), key=lambda x: x[1]['metrics']['accuracy'])\n",
    "best_model_name, best_result = best_model\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"🏆 BEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"   • Accuracy: {best_result['metrics']['accuracy']:.4f} ({best_result['metrics']['accuracy']:.1%})\")\n",
    "print(f\"   • Precision: {best_result['metrics']['precision']:.4f}\")\n",
    "print(f\"   • Recall: {best_result['metrics']['recall']:.4f}\")\n",
    "print(f\"   • F1-Score: {best_result['metrics']['f1']:.4f}\")\n",
    "print(f\"   • Training Time: {best_result['training_time']:.1f} seconds\")\n",
    "print()\n",
    "\n",
    "print(\"📊 COMPLETE RANKING:\")\n",
    "ranked_models = sorted(results.items(), key=lambda x: x[1]['metrics']['accuracy'], reverse=True)\n",
    "for i, (model_name, result) in enumerate(ranked_models, 1):\n",
    "    print(f\"   {i}. {model_name}: {result['metrics']['accuracy']:.1%} accuracy\")\n",
    "print()\n",
    "\n",
    "print(\"💡 RECOMMENDATIONS:\")\n",
    "print(f\"   • For highest accuracy: Use {ranked_models[0][0]}\")\n",
    "print(f\"   • For fastest training: Use {min(results.items(), key=lambda x: x[1]['training_time'])[0]}\")\n",
    "print(f\"   • For production deployment: Consider {best_model_name} for best balance of accuracy and reliability\")\n",
    "print()\n",
    "\n",
    "print(\"🔍 DETAILED INSIGHTS:\")\n",
    "for model_name, result in results.items():\n",
    "    report = result['metrics']['classification_report']\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   • Overall accuracy: {result['metrics']['accuracy']:.1%}\")\n",
    "    print(f\"   • Best class performance: {max(report[cls]['f1-score'] for cls in class_names):.3f} F1-score\")\n",
    "    print(f\"   • Worst class performance: {min(report[cls]['f1-score'] for cls in class_names):.3f} F1-score\")\n",
    "    print(f\"   • Parameters: {sum(p.numel() for p in result['model'].parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HORNET/WASP CLASSIFICATION PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model comparison results to CSV for future reference\n",
    "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"Model comparison results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Optional: Save the best model\n",
    "torch.save(best_result['model'].state_dict(), f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pth')\n",
    "print(f\"Best model ({best_model_name}) saved to 'best_model_{best_model_name.replace(' ', '_').lower()}.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}